{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5661f686-0a9f-4387-b65d-49063221eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------- Cell 1 : Library and Environment Check.\n",
    "import pandas\n",
    "!pip install xgboost\n",
    "!pip install seaborn\n",
    "!pip install openpyxl\n",
    "\n",
    "\n",
    "file_path = '/files/DSAP_Proj_FLFPGrowth/data/raw/children-per-woman-un.csv'\n",
    "Adult_lit_fem_ds = pandas.read_csv(file_path)\n",
    "print(\"File loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60521818-632e-43b3-9bee-e5c4010bc1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "#-------------------------------- Cell 2: Load, Clean, and Filter Data (1991-2021)\n",
    "\n",
    "base_dir = '/files/DSAP_Proj_FLFPGrowth/data/raw' \n",
    "\n",
    "files = {\n",
    "    \"Average Age of Mothers\": 'average-age-of-mothers.csv',\n",
    "    \"Children per Woman\": 'children-per-woman-un.csv',\n",
    "    \"Female Emp/Pop Ratio\": 'modeled-female-employment-to-population-ratio.csv',\n",
    "    \"FLFP Rates\": 'female-labor-force-participation-rates.csv',\n",
    "    \"GDP per Capita\": 'gdp-per-capita-worldbank.csv',\n",
    "    \"Urban Population\": 'urban-and-rural-population.csv',\n",
    "    \"Female Unemployment\": 'unemployment-rate-women.csv',\n",
    "    \"Years of Schooling\": 'years-of-schooling.csv',\n",
    "    \"WBL Index\": 'WBLHistorical.xlsx'\n",
    "}\n",
    "\n",
    "def load_and_clean(path, name):\n",
    "    try:\n",
    "        # 1. Load the file\n",
    "        if path.endswith('.xlsx') or path.endswith('.xls'):\n",
    "            # SPECIAL CASE: Target the specific sheet for WBL\n",
    "            if \"WBL\" in name:\n",
    "                df = pd.read_excel(path, sheet_name=\"WBL Panel 2024\")\n",
    "            else:\n",
    "                df = pd.read_excel(path)\n",
    "        else:\n",
    "            df = pd.read_csv(path)\n",
    "        \n",
    "        # 2. IMMEDIATE CLEANUP of Column Names\n",
    "        df.columns = df.columns.astype(str).str.strip()\n",
    "        \n",
    "        # 3. SPECIAL HANDLING: WBL Index Renaming\n",
    "        if \"WBL\" in name:\n",
    "            if 'Report Year' in df.columns:\n",
    "                df = df.rename(columns={'Report Year': 'Year'})\n",
    "            if 'ISO Code' in df.columns:\n",
    "                df = df.rename(columns={'ISO Code': 'Code'})\n",
    "            elif 'Economy Code' in df.columns:\n",
    "                df = df.rename(columns={'Economy Code': 'Code'})\n",
    "            # The WBL index column itself is usually named 'WBL INDEX'\n",
    "            if 'WBL INDEX' in df.columns:\n",
    "                df = df.rename(columns={'WBL INDEX': 'WBL_Score'})\n",
    "\n",
    "        # --- SPECIAL HANDLING: Average Age of Mothers ---\n",
    "        if name == \"Average Age of Mothers\":\n",
    "            df.columns = df.columns.str.replace('period-', '', regex=False)\n",
    "            \n",
    "        # --- SPECIAL HANDLING: Children per Woman ---\n",
    "        if name == \"Children per Woman\":\n",
    "            long_col = \"Fertility rate - Sex: all - Age: all - Variant: estimates\"\n",
    "            if long_col in df.columns:\n",
    "                df = df.rename(columns={long_col: \"Children per Woman\"})\n",
    "            if 'time' in df.columns:\n",
    "                df['Year'] = df['time']\n",
    "\n",
    "        # 4. Standardize Country Name\n",
    "        if 'Entity' not in df.columns:\n",
    "            if 'Country Name' in df.columns: df = df.rename(columns={'Country Name': 'Entity'})\n",
    "            elif 'Country' in df.columns: df = df.rename(columns={'Country': 'Entity'})\n",
    "            elif 'Economy' in df.columns: df = df.rename(columns={'Economy': 'Entity'})\n",
    "\n",
    "        # 5. Final Check and Year Formatting\n",
    "        if 'Year' in df.columns:\n",
    "            df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n",
    "            df = df.dropna(subset=['Year'])\n",
    "            df['Year'] = df['Year'].astype(int)\n",
    "            \n",
    "            # Filter 1991-2021\n",
    "            df_filtered = df[(df['Year'] >= 1991) & (df['Year'] <= 2021)].copy()\n",
    "            return df_filtered\n",
    "        \n",
    "        else:\n",
    "            print(f\"Error: Still could not find 'Year' in {name}.\")\n",
    "            print(f\"Available columns in sheet: {df.columns.tolist()[:10]}...\") # Print first 10 for debug\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {name}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load all files\n",
    "data_frames = {}\n",
    "print(\"--- Loading Files ---\")\n",
    "for name, filename in files.items():\n",
    "    full_path = os.path.join(base_dir, filename)\n",
    "    data_frames[name] = load_and_clean(full_path, name)\n",
    "\n",
    "# Extract to variables\n",
    "av_age_mothers_df = data_frames[\"Average Age of Mothers\"]\n",
    "child_per_wom_df = data_frames[\"Children per Woman\"]\n",
    "f_emp_pop_ratio_df = data_frames[\"Female Emp/Pop Ratio\"]\n",
    "FLFP_rates_df = data_frames[\"FLFP Rates\"]\n",
    "gdp_per_capita_df = data_frames[\"GDP per Capita\"]\n",
    "LT_urban_pop_df = data_frames[\"Urban Population\"]\n",
    "Unemp_rate_f_df = data_frames[\"Female Unemployment\"]\n",
    "years_school_df = data_frames[\"Years of Schooling\"]\n",
    "wbl_df = data_frames[\"WBL Index\"]\n",
    "\n",
    "# Final Verification\n",
    "print(\"\\n--- Final Row Counts (1991-2021) ---\")\n",
    "for name, df in data_frames.items():\n",
    "    print(f\"{name}: {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5726dd1-72e6-44e5-aaf2-92d655582639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------- Cell 3: Complete Cleaner & Merge ---\n",
    "\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# 1. List of ALL 9 DataFrames\n",
    "dfs_to_merge = [\n",
    "    av_age_mothers_df, \n",
    "    child_per_wom_df, \n",
    "    f_emp_pop_ratio_df, \n",
    "    FLFP_rates_df, \n",
    "    gdp_per_capita_df, \n",
    "    LT_urban_pop_df, \n",
    "    Unemp_rate_f_df,\n",
    "    years_school_df, \n",
    "    wbl_df # <--- Added here\n",
    "]\n",
    "\n",
    "# 2. Define the Cleaning Function\n",
    "def clean_and_prep(df, index):\n",
    "    # Work on a copy\n",
    "    df = df.copy()\n",
    "    \n",
    "    # A. Standardize Column Names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    if 'Country Code' in df.columns: df = df.rename(columns={'Country Code': 'Code'})\n",
    "    if 'Country Name' in df.columns: df = df.rename(columns={'Country Name': 'Entity'})\n",
    "    elif 'Country' in df.columns: df = df.rename(columns={'Country': 'Entity'})\n",
    "        \n",
    "    # B. Clean the Keys (Code and Year)\n",
    "    if 'Code' not in df.columns or 'Year' not in df.columns:\n",
    "        print(f\"Warning: DF #{index} missing 'Code' or 'Year'. Skipping.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Drop rows with missing keys\n",
    "    df = df.dropna(subset=['Code', 'Year'])\n",
    "\n",
    "    # 1. Clean CODE: Force to string\n",
    "    df['Code'] = df['Code'].astype(str).str.strip()\n",
    "    \n",
    "    # 2. Clean YEAR: Force to integer\n",
    "    df['Year'] = pd.to_numeric(df['Year'], errors='coerce')\n",
    "    df = df.dropna(subset=['Year'])\n",
    "    df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "    # C. Remove Duplicates (1 row per Country-Year)\n",
    "    df = df.drop_duplicates(subset=['Code', 'Year'])\n",
    "\n",
    "    # D. Separate Data from Country Name\n",
    "    code_map = {}\n",
    "    if 'Entity' in df.columns:\n",
    "        code_map = df[['Code', 'Entity']].drop_duplicates().set_index('Code')['Entity'].to_dict()\n",
    "        df = df.drop(columns=['Entity']) \n",
    "        \n",
    "    return df, code_map\n",
    "\n",
    "# 3. Initialize the Master DataFrame\n",
    "print(\"--- Starting Full Merge of 9 Files ---\")\n",
    "\n",
    "# Prep the first file\n",
    "df_master, master_code_map = clean_and_prep(dfs_to_merge[0], 0)\n",
    "print(f\"Initialized with {len(df_master)} rows.\")\n",
    "\n",
    "# 4. Loop and Merge the Rest\n",
    "for i in range(1, len(dfs_to_merge)):\n",
    "    # Prep current file\n",
    "    current_df, current_map = clean_and_prep(dfs_to_merge[i], i)\n",
    "    \n",
    "    if current_df is not None:\n",
    "        # Update name list\n",
    "        master_code_map.update(current_map)\n",
    "        \n",
    "        # Merge (Outer Join)\n",
    "        try:\n",
    "            df_master = pd.merge(df_master, current_df, on=['Code', 'Year'], how='outer')\n",
    "        except Exception as e:\n",
    "            print(f\"Merge failed at file #{i}: {e}\")\n",
    "        \n",
    "        # Memory Cleanup\n",
    "        del current_df\n",
    "        gc.collect()\n",
    "\n",
    "# 5. Finish Up\n",
    "# Re-attach Country Names\n",
    "df_master['Country'] = df_master['Code'].map(master_code_map)\n",
    "\n",
    "# Organize Columns\n",
    "cols = ['Country', 'Code', 'Year'] + [c for c in df_master.columns if c not in ['Country', 'Code', 'Year']]\n",
    "df_master = df_master[cols]\n",
    "df_master = df_master.sort_values(by=['Country', 'Year']).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nMerge Complete!\")\n",
    "print(f\"Final Shape: {df_master.shape}\")\n",
    "\n",
    "# 6. Verify GDP Column\n",
    "gdp_cols = [c for c in df_master.columns if \"GDP\" in c]\n",
    "if gdp_cols:\n",
    "    count = df_master[gdp_cols[0]].count()\n",
    "    print(f\"GDP Data Points: {count} (Target: ~6000)\")\n",
    "else:\n",
    "    print(\" GDP Column still missing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8586b-ac8c-4d80-8769-f3b5c1a1f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------- Cell 4: Rename and Organize Columns ---\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Name of Step: Define Mapping\n",
    "rename_map = {\n",
    "    \"WBL_Score\": \"WBL_Legal_Score\",\n",
    "    \"age at childbearing\": \"Mean_Age_Mothers\",\n",
    "    \"Fertility\": \"Fertility_Rate\",         \n",
    "    \"Employment to population\": \"Fem_Emp_Pop_Ratio\",\n",
    "    \"Labor force participation\": \"FLFP_Rate\",\n",
    "    \"GDP per capita\": \"GDP_Per_Capita\",\n",
    "    \"Urban population\": \"Urban_Pop_Rate\",\n",
    "    \"Unemployment, female\": \"Fem_Unemp_Rate\",\n",
    "    \"years of schooling\": \"Years_Schooling\"\n",
    "}\n",
    "\n",
    "# 2. Name of Step: Apply Renaming\n",
    "new_columns = []\n",
    "for col in df_master.columns:\n",
    "    renamed = False\n",
    "    for keyword, short_name in rename_map.items():\n",
    "        if keyword.lower() in str(col).lower():\n",
    "            new_columns.append(short_name)\n",
    "            renamed = True\n",
    "            break\n",
    "    if not renamed:\n",
    "        new_columns.append(col)\n",
    "\n",
    "df_master.columns = new_columns\n",
    "\n",
    "# 3. Name of Step: Define Column Order\n",
    "desired_order = [\n",
    "    'Country', 'Code', 'Year', \n",
    "    'FLFP_Rate', \n",
    "    'WBL_Legal_Score', \n",
    "    'Fem_Emp_Pop_Ratio', \n",
    "    'Fem_Unemp_Rate', \n",
    "    'Mean_Age_Mothers', \n",
    "    'GDP_Per_Capita', \n",
    "    'Urban_Pop_Rate', \n",
    "    'Years_Schooling',\n",
    "    'Fertility_Rate' \n",
    "]\n",
    "\n",
    "# 4. Name of Step: Reorder and Filter\n",
    "existing_cols = [c for c in desired_order if c in df_master.columns]\n",
    "df_master = df_master[existing_cols]\n",
    "\n",
    "# 5. Name of Step: Final Preview\n",
    "print(f\"Final Columns: {df_master.columns.tolist()}\")\n",
    "print(df_master.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be471a-5d4d-421e-bc97-a9228691c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------- Cell 5: Filter Non-Countries, Remove Empty Rows, and Save ---\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define Non-Country Entities to Remove\n",
    "entities_to_drop = [\n",
    "    \"World\",\n",
    "    \"Arab World\", \"European Union\", \"OECD members\",\n",
    "    \"High income\", \"Upper middle income\", \"Middle income\", \n",
    "    \"Lower middle income\", \"Low income\", \"Sub-Saharan Africa\",\n",
    "    \"East Asia & Pacific\", \"Europe & Central Asia\", \n",
    "    \"Latin America & Caribbean\", \"Middle East & North Africa\",\n",
    "    \"North America\", \"South Asia\"\n",
    "]\n",
    "\n",
    "# 2. Define Missing Data Threshold\n",
    "MISSING_THRESHOLD = 0.25 \n",
    "\n",
    "# --- EXECUTE FILTERING ---\n",
    "\n",
    "print(f\"Original Country Count: {df_master['Country'].nunique()}\")\n",
    "\n",
    "# A. Remove known non-country entities\n",
    "df_clean = df_master[~df_master['Country'].isin(entities_to_drop)].copy()\n",
    "\n",
    "# B. Remove \"High Missingness\" Countries\n",
    "data_cols = [c for c in df_clean.columns if c != 'Country']\n",
    "\n",
    "# Calculate % missing for each country (Sum of NaNs / Total Cells)\n",
    "missing_series = df_clean[data_cols].groupby(df_clean['Country']).apply(lambda x: x.isnull().sum().sum() / x.size)\n",
    "\n",
    "bad_countries = missing_series[missing_series > MISSING_THRESHOLD].index.tolist()\n",
    "\n",
    "# Drop them\n",
    "df_clean = df_clean[~df_clean['Country'].isin(bad_countries)]\n",
    "\n",
    "# C. Drop the useless \"OWID\" column if it exists\n",
    "if \"World regions according to OWID\" in df_clean.columns:\n",
    "    df_clean = df_clean.drop(columns=[\"World regions according to OWID\"])\n",
    "\n",
    "# --- SAVE AND SUMMARY ---\n",
    "\n",
    "# Save to CSV\n",
    "output_filename = 'master_dataset_clean.csv'\n",
    "df_clean.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\nFiltering Complete!\")\n",
    "print(f\"Removed Manual List: {len(df_master['Country'].unique()) - len(df_master[~df_master['Country'].isin(entities_to_drop)]['Country'].unique())} entities\")\n",
    "print(f\"Removed High-Missingness Countries: {len(bad_countries)}\")\n",
    "print(f\"   -> Dropped: {bad_countries}\")\n",
    "print(f\"Final Country Count: {df_clean['Country'].nunique()}\")\n",
    "print(f\"Saved to: {output_filename}\")\n",
    "\n",
    "# Final Stats\n",
    "print(\"\\n--- Final Data Completeness ---\")\n",
    "print(df_clean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae4bb0-7a85-44c3-a95b-74124a6c96de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------- Cell 6: Feature Engineering (Lagged) ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Name of Step: Filter 5-Year Intervals\n",
    "years_of_interest = [1991, 1996, 2001, 2006, 2011, 2016, 2021]\n",
    "df_panel = df_clean[df_clean['Year'].isin(years_of_interest)].copy()\n",
    "df_panel = df_panel.sort_values(by=['Code', 'Year'])\n",
    "\n",
    "# 2. Name of Step: Calculate 5-Year Lag for WBL\n",
    "# This uses the legal score from the PREVIOUS 5-year period\n",
    "df_panel['WBL_Lagged'] = df_panel.groupby('Code')['WBL_Legal_Score'].shift(1)\n",
    "\n",
    "# 3. Name of Step: Calculate Target (Growth)\n",
    "df_panel['Next_FLFP'] = df_panel.groupby('Code')['FLFP_Rate'].shift(-1)\n",
    "df_panel['FLFP_Growth_Next_5Y'] = (df_panel['Next_FLFP'] - df_panel['FLFP_Rate']) / df_panel['FLFP_Rate']\n",
    "\n",
    "# 4. Name of Step: Cleanup\n",
    "df_model = df_panel.dropna(subset=['FLFP_Growth_Next_5Y', 'WBL_Lagged']).copy()\n",
    "\n",
    "# 5. Name of Step: Define Features (Using Lagged WBL)\n",
    "X_cols = [\n",
    "    'Fem_Emp_Pop_Ratio', \n",
    "    'Fem_Unemp_Rate', \n",
    "    'Mean_Age_Mothers', \n",
    "    'GDP_Per_Capita', \n",
    "    'Urban_Pop_Rate', \n",
    "    'Years_Schooling', \n",
    "    'Fertility_Rate',\n",
    "    'WBL_Lagged' # <--- Predicting future growth using past laws\n",
    "]\n",
    "y_col = 'FLFP_Growth_Next_5Y'\n",
    "\n",
    "# 6. Name of Step: Temporal Split\n",
    "train_years = [1996, 2001, 2006] # Starts at 1996 because 1991 is now a lag\n",
    "test_years = [2011, 2016]\n",
    "\n",
    "X_train = df_model.loc[df_model['Year'].isin(train_years), X_cols]\n",
    "y_train = df_model.loc[df_model['Year'].isin(train_years), y_col]\n",
    "X_test = df_model.loc[df_model['Year'].isin(test_years), X_cols]\n",
    "y_test = df_model.loc[df_model['Year'].isin(test_years), y_col]\n",
    "\n",
    "print(f\"Ready with {len(X_train)} training and {len(X_test)} test samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf482f-e217-4adb-973d-f300e00c6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------- Cell 7: Model Training and Evaluation ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1. Setup Models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0), # L2 Regularization\n",
    "    \"Lasso Regression\": Lasso(alpha=0.01), # L1 Regularization (Good for feature selection)\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "# 2. Define Validation Strategy\n",
    "# 5-Fold CV: Splits the training data into 5 parts to ensure the model isn't just memorizing\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"--- Model Performance ---\")\n",
    "print(f\"{'Model':<20} | {'CV R2 (Train)':<15} | {'Test RMSE':<12} | {'Test R2':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "# 3. Training Loop\n",
    "for name, model in models.items():\n",
    "    \n",
    "    # Create Pipeline: \n",
    "    # 1. Imputer: Fills any tiny gaps \n",
    "    # 2. Scaler: Standardizes data \n",
    "    # 3. Model: The regressor\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')), \n",
    "        ('scaler', StandardScaler()), \n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # A. Cross-Validation on Training Set (1991-2006)\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='r2')\n",
    "    mean_cv_r2 = cv_scores.mean()\n",
    "    \n",
    "    # B. Train on Full Training Set\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # C. Predict on Test Set (2011-2016)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # D. Calculate Metrics\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"CV_R2\": mean_cv_r2,\n",
    "        \"Test_RMSE\": test_rmse,\n",
    "        \"Test_R2\": test_r2,\n",
    "        \"Pipeline\": pipeline # Save the trained model for later (SHAP)\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:<20} | {mean_cv_r2:.4f}          | {test_rmse:.4f}       | {test_r2:.4f}\")\n",
    "\n",
    "# 4. Save best model for interpretation (sort by Test R2 to find the winner)\n",
    "results_df = pd.DataFrame(results).sort_values(by='Test_R2', ascending=False)\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_pipeline = results_df.iloc[0]['Pipeline']\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f98680-07b1-49c7-82a1-c311d110d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------- Cell 8: Visualize Ridge Coefficients ---\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Extract the model and feature names\n",
    "ridge_model = best_pipeline.named_steps['model']\n",
    "feature_names = X_cols\n",
    "\n",
    "# 2. Get the Coefficients\n",
    "coefficients = ridge_model.coef_\n",
    "\n",
    "# 3. Create a DataFrame for Plotting\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "# 4. Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=coef_df, x='Coefficient', y='Feature', palette='viridis')\n",
    "\n",
    "# Add a vertical line at 0 for clarity\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.title('What Drives FLFP Growth? (Ridge Regression Coefficients)')\n",
    "plt.xlabel('Impact on Future FLFP Growth (Standardized)')\n",
    "plt.ylabel('Predictor Variable')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# 5. Print the exact numbers\n",
    "print(\"--- Feature Impacts (Standardized) ---\")\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a9ea05-68c6-46a9-aca8-9fa0891e5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------- Cell 9: Permutation Importance (Verification) ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# 1. Calculate Permutation Importance\n",
    "perm_importance = permutation_importance(\n",
    "    best_pipeline, X_test, y_test, \n",
    "    n_repeats=30, random_state=42, scoring='r2'\n",
    ")\n",
    "\n",
    "# 2. Organize Data\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "features = np.array(X_cols)\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(\n",
    "    perm_importance.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=features[sorted_idx]\n",
    ")\n",
    "plt.title(\"Permutation Importance (Test Set)\")\n",
    "plt.xlabel(\"Drop in R2 Score (Importance)\")\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Print Summary\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance_Mean': perm_importance.importances_mean,\n",
    "    'Importance_Std': perm_importance.importances_std\n",
    "}).sort_values(by='Importance_Mean', ascending=False)\n",
    "\n",
    "print(\"--- Feature Importance (Ranked) ---\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0bbe4-4931-4d40-96d4-fe537e8eb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------- Cell 10: Partial Dependence Plot (The Convergence Effect) ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# 1. Setup the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# 2. Calculate Partial Dependence\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    best_pipeline, \n",
    "    X_test, \n",
    "    features=['Fem_Emp_Pop_Ratio'], \n",
    "    kind='average',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "plt.title(f\"Convergence Effect: Baseline Employment vs. Predicted Growth\")\n",
    "plt.xlabel(\"Female Employment to Population Ratio (Baseline)\")\n",
    "plt.ylabel(\"Predicted FLFP Growth (Next 5 Years)\")\n",
    "plt.grid(linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ae16b-a5f5-49d4-af02-4af2e6417204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------- Cell 11: Predicted vs. Actual Growth (The \"Reality Check\") ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Predict on the Test Set again\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "# 2. Create a Comparison DataFrame\n",
    "results_df = X_test.copy()\n",
    "results_df['Actual_Growth'] = y_test\n",
    "results_df['Predicted_Growth'] = y_pred\n",
    "results_df['Residual'] = results_df['Actual_Growth'] - results_df['Predicted_Growth']\n",
    "\n",
    "# Add Country and Year back for context (we align by index)\n",
    "results_df['Country'] = df_model.loc[X_test.index, 'Country']\n",
    "results_df['Year'] = df_model.loc[X_test.index, 'Year']\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=results_df, x='Predicted_Growth', y='Actual_Growth', alpha=0.6, color='blue')\n",
    "\n",
    "# Add the perfect prediction line\n",
    "min_val = min(results_df['Actual_Growth'].min(), results_df['Predicted_Growth'].min())\n",
    "max_val = max(results_df['Actual_Growth'].max(), results_df['Predicted_Growth'].max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "plt.title('Reality Check: Did the Model Predict Correctly?')\n",
    "plt.xlabel('Predicted FLFP Growth')\n",
    "plt.ylabel('Actual FLFP Growth')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 4. Highlight the \"Miracle\" Countries (Top Overperformers)\n",
    "overperformers = results_df.sort_values(by='Residual', ascending=False).head(5)\n",
    "\n",
    "print(\"--- Top 5 'Miracle' Countries (Overperformers) ---\")\n",
    "print(overperformers[['Country', 'Year', 'Actual_Growth', 'Predicted_Growth', 'Residual']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f659f09c-b9f8-4aa5-afc4-6711024ce5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check how WBL relates to other predictors\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df_model[X_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation: Is WBL redundant?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529dfff-a0f7-4b20-ba62-3c62891f1605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
